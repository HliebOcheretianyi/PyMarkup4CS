import spacy
import pickle
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from spacy.lang.uk.stop_words import STOP_WORDS
import pandas as pd
from paths import *

class SimpleGrammarNER:
    def __init__(self, model_name="uk_core_news_md", pickle_path=f'{ML_FOLDER}/NLPickles/ner_components.pkl'):
        self.nlp = spacy.load(model_name)
        self.pickle_path = pickle_path

        with open(self.pickle_path, 'rb') as f:
            components = pickle.load(f)

        self.semantic_seeds = components['semantic_seeds']
        self.vectorizer = components['vectorizer']
        self.tfidf_matrix = components['tfidf_matrix']
        self.entity_vectors = components['entity_vectors']

        # raw_semantic_seeds = [
        #     "–∫–æ–¥",
        #     "—ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä",
        #     "–Ω–æ–º–µ—Ä",
        #     "—É–Ω—ñ–∫–∞–ª—å–Ω–∏–π –Ω–æ–º–µ—Ä",
        #     "—Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π –Ω–æ–º–µ—Ä",
        #     "—Å–µ—Ä—ñ–π–Ω–∏–π –Ω–æ–º–µ—Ä",
        #     "—Å—Ç—Ä–∞—Ö–æ–≤–∏–π –∫–æ–¥",
        #     "–∫–æ–¥ —Ç–∏–ø—É —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è",
        #     "—Ç–∏–ø –ø—Ä–æ–≥—Ä–∞–º–∏",
        #     "–∫–æ–¥ –ø—Ä–æ–≥—Ä–∞–º–∏",
        #     "–ø—Ä–æ–≥—Ä–∞–º–∞",
        #     "—Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è",
        #     "–≤–∏–¥ —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è",
        #     "—Å—Ç—Ä–∞—Ö–æ–≤–∏–π –ø–æ–ª—ñ—Å",
        #     "—Å—Ç—Ä–∞—Ö–æ–≤–∏–π —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç",
        #     "—Å—Ç—Ä–∞—Ö–æ–≤–∞ –∫–æ–º–ø–∞–Ω—ñ—è",
        #     "–¥–æ–≥–æ–≤—ñ—Ä —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è",
        #     "—Å—Ç—Ä–∞—Ö–æ–≤–∏–π –ø–∞–∫–µ—Ç",
        #     "—Å—Ç—Ä–∞—Ö–æ–≤–∏–π –ø–ª–∞–Ω",
        #     "–ø–æ–ª—ñ—Å",
        #     "–ø–æ–∫—Ä–∏—Ç—Ç—è",
        #     "–¥–æ–∫—É–º–µ–Ω—Ç",
        #     "–ø–∞—Å–ø–æ—Ä—Ç",
        #     "—Å–≤—ñ–¥–æ—Ü—Ç–≤–æ",
        #     "–¥–æ–≤—ñ–¥–∫–∞",
        #     "–ª—ñ—Ü–µ–Ω–∑—ñ—è",
        #     "–ø–æ—Å–≤—ñ–¥—á–µ–Ω–Ω—è",
        #     "–≤–∏–ø–∏—Å–∫–∞",
        #     "–∑–∞—è–≤–∞",
        #     "—É–≥–æ–¥–∞",
        #     "–∞–∫—Ç",
        #     "—Ä–µ—î—Å—Ç—Ä",
        #     "—Ñ–æ—Ä–º–∞",
        #     "–∞–Ω–∫–µ—Ç–∞"
        # ]
        #
        # semantic = []
        # for word in raw_semantic_seeds:
        #     doc = self.nlp(word)
        #     lemmas = " ".join([token.lemma_ for token in doc])
        #     semantic.append(lemmas)
        #
        # # –ù–∞—Å—ñ–Ω–Ω—î–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
        # self.semantic_seeds = {
        #     "DOCUMENT CODE": semantic
        # }
        #
        # # –ü–æ–±—É–¥—É—î–º–æ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ seed-—Å–ª–æ–≤–∞ –æ–∫—Ä–µ–º–æ
        # self.vectorizer = TfidfVectorizer()
        # self.tfidf_matrix = self.vectorizer.fit_transform(semantic)
        #
        # self.entity_vectors = {}
        # for i, seed in enumerate(semantic):
        #     self.entity_vectors[seed] = self.tfidf_matrix[i]

    def save_to_pickle(self):
        components = {
            'semantic_seeds': self.semantic_seeds,
            'vectorizer': self.vectorizer,
            'tfidf_matrix': self.tfidf_matrix,
            'entity_vectors': self.entity_vectors,
            # Note: We don't pickle the spaCy model as it's better loaded fresh
        }

        with open(self.pickle_path, 'wb') as f:
            pickle.dump(components, f)
        print(f"Components saved to {self.pickle_path}")

    def calculate_similarity(self, text, entity_type):
        vec = self.vectorizer.transform([text])
        max_similarity = 0
        # Find max similarity with any seed word for this entity type
        for seed in self.semantic_seeds[entity_type]:
            if seed in self.entity_vectors:
                similarity = cosine_similarity(vec, self.entity_vectors[seed])[0, 0]
                max_similarity = max(max_similarity, similarity)
        return max_similarity

    def extract_entities(self, text, threshold=0.3):
        text = " ".join([word for word in text.split() if word.lower() not in STOP_WORDS])
        text = text.strip()
        text = text.replace('"', '').replace("/", "").replace('\\', "").replace(',', '').replace('.', '')
        text = " ".join(text.split())
        doc = self.nlp(text)

        candidates = []

        for i, token in enumerate(doc):
            # NUM + NOUN (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "521 –∫–æ–¥")
            if i < len(doc) - 1 and token.pos_ == "NUM" and doc[i + 1].pos_ == "NOUN":
                candidates.append(f"{token.text} {doc[i + 1].lemma_}")

            # NOUN + NUM NUM NUM... (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "–∫–æ–¥ 521 322 18")
            if token.pos_ == "NOUN" and token.lemma_.lower() in {"–∫–æ–¥", "–¥–æ–∫—É–º–µ–Ω—Ç", "–Ω–æ–º–µ—Ä", "–ø—Ä–æ–≥—Ä–∞–º–∞", "—Ç–∏–ø", "–ø–æ–ª—ñ—Å",
                                                                 "—Ñ–æ—Ä–º–∞"}:
                nums = []
                j = i + 1
                # Only take numbers, skip other words
                while j < len(doc) and doc[j].pos_ == "NUM" and doc[j].text.isdigit():
                    nums.append(doc[j].text)
                    j += 1
                if len(nums) >= 1:  # At least one number
                    cand = f"{token.lemma_} {' '.join(nums)}"
                    candidates.append(cand)

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–æ—é
        entities = []
        for cand in candidates:
            # Skip candidates without numbers
            if not any(char.isdigit() for char in cand):
                continue

            best_label, best_score = None, 0
            for label in self.semantic_seeds.keys():
                sim = self.calculate_similarity(cand.lower(), label)
                if sim > best_score:
                    best_label, best_score = label, sim
            if best_score >= threshold:
                entities.append((cand, best_label, best_score))

        return entities


# üîπ –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
if __name__ == "__main__":
    ner = SimpleGrammarNER()

    text = '–ö–æ–¥ –ø–æ–≤–µ—Ä—Ç–∞—î —Ç–µ–∫—Å—Ç–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ–∫—Ä–∏—Ç—Ç—è –∑–∞ —É–º–æ–≤–æ—é ¬´–í–û–Ñ–ù–ù–Ü –î–Ü–á¬ª –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–∏ –∑ –∫–æ–¥–æ–º —Ç–∏–ø—É —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è "211", "212", "213", "214", "221", "222", "223" –∞–±–æ "224", —è–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä. –Ø–∫—â–æ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ ‚Äî ¬´–í —Ä–æ–∑–º—ñ—Ä—ñ 15% –≤—ñ–¥ —Å—Ç—Ä–∞—Ö–æ–≤–æ—ó —Å—É–º–∏, –∞–ª–µ –Ω–µ –±—ñ–ª—å—à–µ 300 000 –≥—Ä–Ω¬ª, —Ä–µ–∑—É–ª—å—Ç–∞—Ç:¬´–¥–æ—Ä—ñ–≤–Ω—é—î 15% –≤—ñ–¥ —Å—Ç—Ä–∞—Ö–æ–≤–æ—ó —Å—É–º–∏, –∞–ª–µ –Ω–µ –±—ñ–ª—å—à–µ 300 000,00 –≥—Ä–Ω –∞–≥—Ä–µ–≥–∞—Ç–Ω–æ –∑–∞ –î–æ–≥–æ–≤–æ—Ä–æ–º –∑–∞ —Ü—ñ—î—é —É–º–æ–≤–æ—é¬ª.–Ø–∫—â–æ –∑–Ω–∞—á–µ–Ω–Ω—è ‚Äî ¬´–í —Ä–æ–∑–º—ñ—Ä—ñ –ø–æ–≤–Ω–æ—ó –≤–∞—Ä—Ç–æ—Å—Ç—ñ –¢–ó, –∞–ª–µ –Ω–µ –±—ñ–ª—å—à–µ 2 –º–ª–Ω –≥—Ä–Ω¬ª, —Ä–µ–∑—É–ª—å—Ç–∞—Ç:¬´–¥–æ—Ä—ñ–≤–Ω—é—î —Å—Ç—Ä–∞—Ö–æ–≤—ñ–π —Å—É–º—ñ, –∑–∞–∑–Ω–∞—á–µ–Ω—ñ–π –≤ –ø. 6.3 –ß–∞—Å—Ç–∏–Ω–∏ 1 –î–æ–≥–æ–≤–æ—Ä—É, –∞–ª–µ –Ω–µ –±—ñ–ª—å—à–µ 2 000 000,00 –≥—Ä–Ω¬ª.–Ø–∫—â–æ –ø—Ä–æ–≥—Ä–∞–º–∞ –∞–±–æ –ø–∞—Ä–∞–º–µ—Ç—Ä –≤—ñ–¥—Å—É—Ç–Ω—ñ ‚Äî –ø–æ–≤–µ—Ä—Ç–∞—î "---".'
    ents = []
    # ents.extend(ner.extract_entities(text))
    # for e in ents:
    #     print(f"{e[0]} -> {e[1]} (similarity {e[2]:.2f})")
    # print("–ó–Ω–∞–π–¥–µ–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ:", ents)

    df = pd.read_excel(f'{DATA_FOLDER}/DocProperties/DocProperties_NEW_filled.xlsx')
    for line in df['Description']:
        ents.extend(ner.extract_entities(line))

    for e in ents:
        print(f"{e[0]} -> {e[1]} (similarity {e[2]:.2f})")