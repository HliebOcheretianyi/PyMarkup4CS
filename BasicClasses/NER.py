import spacy
import pickle
from spacy import displacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from spacy.lang.uk.stop_words import STOP_WORDS
import pandas as pd
from paths import *

class SimpleGrammarNER:
    def __init__(self, model_name="uk_core_news_sm", pickle_path=f'{ML_FOLDER}/NLPickles/ner_components.pkl'):
        self.nlp = spacy.load(model_name)
        self.pickle_path = pickle_path

        with open(self.pickle_path, 'rb') as f:
            components = pickle.load(f)

        self.semantic_seeds = components['semantic_seeds']
        self.vectorizer = components['vectorizer']
        self.tfidf_matrix = components['tfidf_matrix']
        self.entity_vectors = components['entity_vectors']

        # raw_semantic_seeds = [
        #     "–∫–æ–¥",
        #     "—ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä",
        #     "–Ω–æ–º–µ—Ä",
        #     "—Å—Ç—Ä–∞—Ö–æ–≤–∏–π –∫–æ–¥",
        #     "—Ç–∏–ø –ø—Ä–æ–≥—Ä–∞–º–∏",
        #     "–∫–æ–¥ —Ç–∏–ø—É —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è",
        #     "—Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è",
        #     "–ø—Ä–æ–≥—Ä–∞–º–∞"
        # ]
        # semantic = []
        # for word in raw_semantic_seeds:
        #     doc = self.nlp(word)
        #     lemmas = " ".join([token.lemma_ for token in doc])
        #     semantic.append(lemmas)
        #
        # # –ù–∞—Å—ñ–Ω–Ω—î–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
        # self.semantic_seeds = {
        #     "DOCUMENT CODE": semantic
        # }
        #
        # # –ü–æ–±—É–¥—É—î–º–æ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏ —Ç—ñ–ª—å–∫–∏ –¥–ª—è seed-—Å–ª–æ–≤
        # all_texts = [" ".join(seeds) for seeds in self.semantic_seeds.values()]
        # self.vectorizer = TfidfVectorizer()
        # self.tfidf_matrix = self.vectorizer.fit_transform(all_texts)
        #
        # self.entity_vectors = {}
        # for i, label in enumerate(self.semantic_seeds.keys()):
        #     self.entity_vectors[label] = self.tfidf_matrix[i]

    def save_to_pickle(self):
        components = {
            'semantic_seeds': self.semantic_seeds,
            'vectorizer': self.vectorizer,
            'tfidf_matrix': self.tfidf_matrix,
            'entity_vectors': self.entity_vectors,
            # Note: We don't pickle the spaCy model as it's better loaded fresh
        }

        with open(self.pickle_path, 'wb') as f:
            pickle.dump(components, f)
        print(f"Components saved to {self.pickle_path}")

    def calculate_similarity(self, text, entity_type):
        vec = self.vectorizer.transform([text])
        return cosine_similarity(vec, self.entity_vectors[entity_type])[0, 0]

    def extract_entities(self, text, threshold=0.2):
        text = " ".join([word for word in text.split() if word.lower() not in STOP_WORDS])
        text = text.strip()
        text = text.replace('"', '').replace("/", "").replace('\\', "").replace(',', '').replace('.', '')
        text = " ".join(text.split())
        doc = self.nlp(text)

        candidates = []

        # for i, token in enumerate(doc):
            # # –í–∏–ø–∞–¥–æ–∫ PROPN (—ñ–º–µ–Ω–∞, –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –Ω–∞–∑–≤–∏)
            # if token.pos_ == "PROPN":
            #     candidates.append(token.text)
            #
            # # –í–∏–ø–∞–¥–æ–∫ ADJ+NOUN
            # if i < len(doc) - 1 and token.pos_ == "ADJ" and doc[i+1].pos_ == "NOUN":
            #     candidates.append(f"{token.text} {doc[i+1].text}")

        for i, token in enumerate(doc):
            # NUM + NOUN (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "521 –∫–æ–¥")
            if i < len(doc) - 1 and token.pos_ == "NUM" and doc[i + 1].pos_ == "NOUN":
                candidates.append(f"{token.text} {doc[i + 1].lemma_}")

            # NOUN + NUM NUM NUM... (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "–∫–æ–¥ 521 322 18")
            if token.pos_ == "NOUN":
                nums = []
                j = i + 1
                while j < len(doc) and doc[j].pos_ == "NUM":
                    nums.append(doc[j].text)
                    j += 1
                if nums:
                    cand = f"{token.lemma_} {' '.join(nums)}"
                    candidates.append(cand)

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–æ—é
        entities = []
        for cand in candidates:
            best_label, best_score = None, 0
            for label in self.semantic_seeds.keys():
                sim = self.calculate_similarity(cand.lower(), label)
                if sim > best_score:
                    best_label, best_score = label, sim
            if best_score >= threshold:
                entities.append((cand, best_label, best_score))

        return entities


# üîπ –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
if __name__ == "__main__":
    ner = SimpleGrammarNER()
    text = '–ö–æ–¥ –ø–æ–≤–µ—Ä—Ç–∞—î —Ç–µ–∫—Å—Ç–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ–∫—Ä–∏—Ç—Ç—è –∑–∞ —É–º–æ–≤–æ—é ¬´–í–û–Ñ–ù–ù–Ü –î–Ü–á¬ª –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–∏ –∑ –∫–æ–¥–æ–º —Ç–∏–ø—É —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è "211", "212", "213", "214", "221", "222", "223" –∞–±–æ "224", —è–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä. –Ø–∫—â–æ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ ‚Äî ¬´–í —Ä–æ–∑–º—ñ—Ä—ñ 15% –≤—ñ–¥ —Å—Ç—Ä–∞—Ö–æ–≤–æ—ó —Å—É–º–∏, –∞–ª–µ –Ω–µ –±—ñ–ª—å—à–µ 300 000 –≥—Ä–Ω¬ª, —Ä–µ–∑—É–ª—å—Ç–∞—Ç:¬´–¥–æ—Ä—ñ–≤–Ω—é—î 15% –≤—ñ–¥ —Å—Ç—Ä–∞—Ö–æ–≤–æ—ó —Å—É–º–∏, –∞–ª–µ –Ω–µ –±—ñ–ª—å—à–µ 300 000,00 –≥—Ä–Ω –∞–≥—Ä–µ–≥–∞—Ç–Ω–æ –∑–∞ –î–æ–≥–æ–≤–æ—Ä–æ–º –∑–∞ —Ü—ñ—î—é —É–º–æ–≤–æ—é¬ª.–Ø–∫—â–æ –∑–Ω–∞—á–µ–Ω–Ω—è ‚Äî ¬´–í —Ä–æ–∑–º—ñ—Ä—ñ –ø–æ–≤–Ω–æ—ó –≤–∞—Ä—Ç–æ—Å—Ç—ñ –¢–ó, –∞–ª–µ –Ω–µ –±—ñ–ª—å—à–µ 2 –º–ª–Ω –≥—Ä–Ω¬ª, —Ä–µ–∑—É–ª—å—Ç–∞—Ç:¬´–¥–æ—Ä—ñ–≤–Ω—é—î —Å—Ç—Ä–∞—Ö–æ–≤—ñ–π —Å—É–º—ñ, –∑–∞–∑–Ω–∞—á–µ–Ω—ñ–π –≤ –ø. 6.3 –ß–∞—Å—Ç–∏–Ω–∏ 1 –î–æ–≥–æ–≤–æ—Ä—É, –∞–ª–µ –Ω–µ –±—ñ–ª—å—à–µ 2 000 000,00 –≥—Ä–Ω¬ª.–Ø–∫—â–æ –ø—Ä–æ–≥—Ä–∞–º–∞ –∞–±–æ –ø–∞—Ä–∞–º–µ—Ç—Ä –≤—ñ–¥—Å—É—Ç–Ω—ñ ‚Äî –ø–æ–≤–µ—Ä—Ç–∞—î "---".'
    ents = []
    # ents.extend(ner.extract_entities(text))
    # for e in ents:
    #     print(f"{e[0]} -> {e[1]} (similarity {e[2]:.2f})")
    # print("–ó–Ω–∞–π–¥–µ–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ:", ents)

    df = pd.read_excel(f'{DATA_FOLDER}/DocProperties/DocProperties_NEW_filled.xlsx')
    for line in df['Description']:
        ents.extend(ner.extract_entities(line))

    for e in ents:
        print(f"{e[0]} -> {e[1]} (similarity {e[2]:.2f})")
